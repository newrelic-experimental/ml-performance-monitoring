{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5096ee85-a0d8-40aa-abf1-406ccee32daf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New Relic ML Performance Monitoring- Bring Your Own Data\n",
    "\n",
    "[ml-performance-monitoring](https://github.com/newrelic-experimental/ml-performance-monitoring) provides a Python library for sending machine learning models' inference data and performance metrics into New Relic.\n",
    "<br>\n",
    "It is based on the [newrelic-telemetry-sdk-python](https://github.com/newrelic/newrelic-telemetry-sdk-python) library.\n",
    "<br>\n",
    "By using this package, you can easily and quickly monitor your model, directly from a Jupyter notebook or any other environment.\n",
    " <br>\n",
    "This notebook provides an example of sending inference data and metrics of an RandomForestClassifier model\n",
    "\n",
    "<U>Note</U>- this notebook uses the libraries:\n",
    "* numpy\n",
    "* pandas\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf23f3-4ab0-4e52-9c3a-5c150590cc39",
   "metadata": {},
   "source": [
    "### 1. Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ee9c03-5043-4a26-94e6-5924cf549cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from ml_performance_monitoring.monitor import MLPerformanceMonitoring, wrap_model\n",
    "from ml_performance_monitoring.psi import calculate_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432798ef-1fb1-4006-9510-84c4778454a9",
   "metadata": {},
   "source": [
    "### 2. Load the Iris dataset and split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302cf098-c363-4989-90a7-1947340eebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2]]),\n",
       " array([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "X, y = (\n",
    "    iris_dataset[\"data\"],\n",
    "    iris_dataset[\"target\"],\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212217d-1573-4ccd-98e0-3725800d8274",
   "metadata": {},
   "source": [
    "### 3. Fitting Random Forest Classification model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb62a1bb-28ea-4105-93be-62d0a4c8deaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectKBest(k=2, score_func=<function chi2 at 0x13e0fa5e0>)),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set up a pipeline with a feature selection preprocessor that\n",
    "# selects the top 2 features to use.\n",
    "# The pipeline then uses a RandomForestClassifier to train the model.\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"feature_selection\", SelectKBest(chi2, k=2)),\n",
    "        (\"classification\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5c717-58d3-4b41-88c3-549a044d7d7d",
   "metadata": {},
   "source": [
    "### 4. Predicting the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcced287-1691-4fd3-a802-bea65f5edf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 1, 2,\n",
       "       1, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 1, 1, 2, 1, 0, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 1, 1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66de2b-e106-4a81-a5f8-83fb252e55fd",
   "metadata": {},
   "source": [
    "### 5. Record inference data to New Relic\n",
    "<b> You have two options for sending your model's inference data (features and predictions) to New Relic: </b>\n",
    "<br> 1. \"Online\" instrumentation - sending the data while the model is being invoked in production\n",
    "<br> 2. \"Offline\" instrumentation - sending the data as a dataset (as an np.array or pandas dataframe) \n",
    "\n",
    "<b>   The MLPerformanceMonitoring object requires the following parameters: </b>\n",
    "<br> 1. <b>model_name</b> - must be unique per model\n",
    "<br> 2. <b>New Relic insert key </b>-  can be license-key or insights-insert-key [how to get your insert key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#insights-insert-key), set it as the following environment variable: NEW_RELIC_INSERT_KEY <br>\n",
    "Optional parameters:\n",
    "<br> \n",
    "3. <b>metadata</b> (dictionary) - will be added to each event (row) of the data \n",
    "<br>\n",
    "4. <b>send_data_metrics </b>(boolean) - send data metrics (statistics) to New Relic (False as default)\n",
    "<br> \n",
    "5. <b>features_columns</b> (list) - the features' names ordered as X columns. On New Relic data, the names will be prefixed with the string 'feature_'\n",
    "<br>\n",
    "6. <b> labels_columns</b> (list) - the labels' names ordered as y columns. On New Relic data, the names will be prefixed with the string 'label_'\n",
    "<br> (note: The parameters features_columns and labels_columns are only relevant when sending the data as an np.array. When the data is sent as a dataframe, the dataframes (X,y) columns' names will be taken as features and labels names respectively. In addition, if you send your data as an np.array without sending the features_columns and labels_columns, on New Relic data, the names will appear as \"feature_{n}\" and \"lablel_{n}\" numbered by the features/labels order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74dc293-03cb-4e05-99c2-fc88a3c513a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"environment\": \"aws\", \"dataset\": \"Iris\", \"version\": \"1.0\"}\n",
    "features_columns = [\n",
    "    \"sepal_length\",\n",
    "    \"sepal_width\",\n",
    "    \"petal_length\",\n",
    "    \"petal_width\",\n",
    "]\n",
    "\n",
    "\n",
    "labels_columns = [\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ee01e-c2ad-4c38-a870-6017d065a7e6",
   "metadata": {},
   "source": [
    "<b> 5.1.  \"Online\" instrumentation </b>\n",
    "<br>\n",
    "The wrap_model function extends the model/pipeline methods with the functionality of sending the inference data as [custom event](https://docs.newrelic.com/docs/data-apis/ingest-apis/introduction-event-api/) named \"InferenceData\" to New Relic NRDB.\n",
    "Wrap your model or pipeline by sending it as a parameter and then use it (the return value) as usual (fit, predict, etc.). Your inference data and data metrics will be sent automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f58174-be3d-4801-861f-1b53797f413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syehezkel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/ml_performance_monitoring/monitor.py:353: UserWarning: send_data_metrics occurs only when there are at least 100 rows. To change this constraint please update the variable 'data_summary_min_rows' in the fuction call\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    }
   ],
   "source": [
    "ml_performence_monitor_pipeline = wrap_model(\n",
    "    insert_key=None,  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here,\n",
    "    model=pipeline,\n",
    "    model_name=\"RandomForestClassifier on Iris Dataset\",\n",
    "    metadata=metadata,\n",
    "    send_data_metrics=True,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    "    label_type=\"categorical\",\n",
    ")\n",
    "\n",
    "y_pred = ml_performence_monitor_pipeline.predict(\n",
    "    X=X_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33a366-ab59-4b87-9a2e-0900cfc3a1cf",
   "metadata": {},
   "source": [
    "<b> 5.2.  \"Offline\" instrumentation </b>\n",
    "<br>\n",
    "Define an MLPerformanceMonitoring object and send your inference data and data metrics as an np.array or as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8638b81c-a85b-4901-a7ec-9f46b84dd259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syehezkel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/ml_performance_monitoring/monitor.py:68: UserWarning: model wasn't defined, please use 'record_inference_data' to send data\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_monitor = MLPerformanceMonitoring(\n",
    "    insert_key=\"NRII-t2xSb8p8EDY3foKtS4-dysBFuqDCxf4X\",  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here,\n",
    "    model_name=\"RandomForestClassifier on Iris Dataset\",\n",
    "    metadata=metadata,\n",
    "    send_data_metrics=True,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    "    label_type=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6f13d-83f4-43e3-ab8c-8f027e4fa723",
   "metadata": {},
   "source": [
    "<b> 5.2.1  Send your features and predictions as an np.array. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4741c115-f94f-4867-9249-17fc39119089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    }
   ],
   "source": [
    "ml_monitor.record_inference_data(X=X_test, y=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07124c7-98b6-45fc-83c8-ae8a1d7d7ee8",
   "metadata": {},
   "source": [
    "<b> 5.2.2  Send your features and prediction as a pd.DataFrame. <br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a8a252-675f-4f17-a325-6c940a0ad4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.8          4.0           1.2          0.2\n",
       "1           5.1          2.5           3.0          1.1\n",
       "2           6.6          3.0           4.4          1.4\n",
       "3           5.4          3.9           1.3          0.4\n",
       "4           7.9          3.8           6.4          2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.DataFrame(\n",
    "    list(map(np.ravel, X_test)),\n",
    "    columns=features_columns,\n",
    ")\n",
    "\n",
    "y_pred_df = pd.DataFrame(\n",
    "    list(map(np.ravel, y_pred)),\n",
    "    columns=labels_columns,\n",
    ")\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f58775-0e59-4689-8c9c-48a8704ad0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species\n",
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf674cc-835e-4ea1-9402-20fd689b2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference data sent successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syehezkel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/ml_performance_monitoring/monitor.py:353: UserWarning: send_data_metrics occurs only when there are at least 100 rows. To change this constraint please update the variable 'data_summary_min_rows' in the fuction call\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ml_monitor.record_inference_data(X=X_df, y=y_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd79a4-335e-46f1-b28b-b0e4da4081ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Record metrics to New Relic\n",
    "You can stream custom metrics to New Relic, monitoring your model performance, model data and drift metrics. These metrics will be sent to NRDB as [metric data](https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/introduction-metric-api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dae00f-bafe-46e2-9b79-11a561e3ea86",
   "metadata": {},
   "source": [
    "<b> 6.1.  model performance metrics</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc20d627-9357-498f-a609-81636bb20358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    : 0.9466666666666667\n",
      "Recall      : 0.9466666666666667\n",
      "Precision   : 0.9486769230769231\n",
      "F1 Score    : 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "ac_sc = accuracy_score(y_test, y_pred)\n",
    "rc_sc = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "pr_sc = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "f1_sc = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Accuracy    : {ac_sc}\")\n",
    "print(f\"Recall      : {rc_sc}\")\n",
    "print(f\"Precision   : {pr_sc}\")\n",
    "print(f\"F1 Score    : {f1_sc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36710290-5eae-4bc3-b04b-f77a26c053ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_metric sent successfully\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"Accuracy\": ac_sc,\n",
    "    \"Recall\": rc_sc,\n",
    "    \"Precision\": pr_sc,\n",
    "    \"F1_Score\": f1_sc,\n",
    "}\n",
    "metrics\n",
    "ml_monitor.record_metrics(metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69928d-e76f-4dbe-9827-906dcd110089",
   "metadata": {},
   "source": [
    "<b> 6.2.  drift metrics</b>\n",
    "<br>\n",
    "send your model drift metric of data drift mertics (need to add the feature_name as variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8250a-0ab5-487a-abd9-fd50a87a3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate psi for top features\n",
    "df_validation = np.transpose(X_test)\n",
    "df_training = np.transpose(X_train)\n",
    "top_feature_list = [0, 1, 2, 3]\n",
    "data_drift_metrics = {}\n",
    "psi_list = []\n",
    "for index, feature_name in enumerate(features_columns):\n",
    "    # Assuming you have a validation and training set\n",
    "    psi_t = calculate_psi(\n",
    "        df_validation[index],\n",
    "        df_training[index],\n",
    "        buckettype=\"quantiles\",\n",
    "        buckets=10,\n",
    "        axis=1,\n",
    "    )\n",
    "    ml_monitor.record_metrics(metrics={\"data_drift\": psi_t}, feature_name=feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2d631b4-5c25-4f35-a601-6a3d536670bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_metric sent successfully\n"
     ]
    }
   ],
   "source": [
    "model_drift = calculate_psi(y_pred, y_train, buckettype=\"quantiles\", buckets=10, axis=1)\n",
    "ml_monitor.record_metrics(metrics={\"model_drift\": model_drift})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f3475-dba8-4529-8de5-f770d3d44924",
   "metadata": {},
   "source": [
    "### 7. Optional - Simulate 24 hours of model inference data  \n",
    "As written, the main purpose of this library is to record inference data of a scheduled model in production. By running the cell below, a simulation of inference data of the RandomForest Classifier model on the Iris Dataset will run each hour in the last 24 hours. After running the cell, you can view the data in 2 different places:<br>\n",
    "* **Machine learning model** entity- an entity of the type **machine learning model** is automatically created. You can explore your model entities by selecting **Explorer** on [New Relic One](https://one.newrelic.com/launcher/), and going to the **Machine Learning** section on the left navigation menu.\n",
    "\n",
    "* Example dashboard-follow the [instructions](https://docs.newrelic.com/docs/alerts-applied-intelligence/mlops/bring-your-own/mlops-byo/#use-case) to view the data in the example dashboard.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c8657-b4b4-4d49-9582-248d5a1673a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=10, criterion=\"entropy\", random_state=0\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "ml_monitor = MLPerformanceMonitoring(\n",
    "    insert_key=None,  # set the environment variable NEW_RELIC_INSERT_KEY or send your insert key here,\n",
    "    model_name=\"RandomForest Classifier on Iris Dataset - Inference Simulation\",\n",
    "    metadata=metadata,\n",
    "    features_columns=features_columns,\n",
    "    labels_columns=labels_columns,\n",
    "    label_type=\"categorical\",\n",
    ")\n",
    "\n",
    "last_24h_date = lastHourDateTime = datetime.now() - timedelta(hours=24)\n",
    "last_24h_timestamp = int(datetime.timestamp(last_24h_date) * 1000)\n",
    "\n",
    "for i in range(24):\n",
    "\n",
    "    idx = np.random.choice(np.arange(len(X)), 10, replace=False)\n",
    "    X_sample = X[idx]\n",
    "    y_sample = y[idx]\n",
    "\n",
    "    y_pred = classifier.predict(X_sample)\n",
    "\n",
    "    X_df = pd.DataFrame(\n",
    "        list(map(np.ravel, X_sample)),\n",
    "        columns=features_columns,\n",
    "    )\n",
    "\n",
    "    y_pred_df = pd.DataFrame(\n",
    "        list(map(np.ravel, y_pred)),\n",
    "        columns=labels_columns,\n",
    "    )\n",
    "\n",
    "    y_pred_df.loc[y_pred_df.species == 0, \"species\"] = \"Setosa\"\n",
    "    y_pred_df.loc[y_pred_df.species == 1, \"species\"] = \"Versicolour\"\n",
    "    y_pred_df.loc[y_pred_df.species == 2, \"species\"] = \"Virginica\"\n",
    "\n",
    "    ml_monitor.record_inference_data(\n",
    "        X=X_df, y=y_pred_df, timestamp=last_24h_timestamp + i * 3600000\n",
    "    )\n",
    "\n",
    "    # Model Evaluation\n",
    "    ac_sc = accuracy_score(y_sample, y_pred)\n",
    "    rc_sc = recall_score(y_sample, y_pred, average=\"weighted\")\n",
    "    pr_sc = precision_score(y_sample, y_pred, average=\"weighted\")\n",
    "    f1_sc = f1_score(y_sample, y_pred, average=\"micro\")\n",
    "\n",
    "    ## Calculate psi for top features\n",
    "    df_validation = np.transpose(X_test)\n",
    "    df_training = np.transpose(X_train)\n",
    "    top_feature_list = [0, 1, 2, 3]\n",
    "    data_drift_metrics = {}\n",
    "    psi_list = []\n",
    "    for index, feature_name in enumerate(features_columns):\n",
    "        # Assuming you have a validation and training set\n",
    "        psi_t = calculate_psi(\n",
    "            df_validation[index],\n",
    "            df_training[index],\n",
    "            buckettype=\"quantiles\",\n",
    "            buckets=10,\n",
    "            axis=1,\n",
    "        )\n",
    "        # ml_monitor.record_metrics(metrics={\"data_drift\":psi_t},feature_name=feature_name)\n",
    "\n",
    "    model_drift = calculate_psi(\n",
    "        y_pred, y_train, buckettype=\"quantiles\", buckets=10, axis=1\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": ac_sc,\n",
    "        \"Recall\": rc_sc,\n",
    "        \"Precision\": pr_sc,\n",
    "        \"F1_Score\": f1_sc,\n",
    "        \"model_drift\": model_drift,\n",
    "    }\n",
    "    metrics\n",
    "    ml_monitor.record_metrics(metrics=metrics)\n",
    "\n",
    "    X_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
